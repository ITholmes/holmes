# 搭建大数据集群架构

步骤：数据采集 → Hadoop HDFS 存储 → HBase 缓存 → 使用 Spark 和 Flink 进行数据处理和分析。

# 1. 搭建Hadoop集群

虚拟机角色分配
VM 1：主节点，运行 NameNode 和 ResourceManager。
VM 2：从节点，运行 DataNode 和 NodeManager。
VM 3：从节点，运行 DataNode 和 NodeManager。

hadoop版本：hadoop-3.2.4



1. 准备 JDK 和 配置网络
下载并安装JDK：
```shell
# 检查是否安装了java，若没有需要先安装JDK，注意版本一般都是1.8
java -version 
# 安装可以直接去官方下载对应的压缩包，还可以通过yum or apt-get命令安装等等。
```

配置静态网络：
```shell

```

配置 `/etc/hosts` 文件：
```shell

```




2. 下载和安装hadoop。
```shell
cd /opt
# 选择合适的hadoop版本
wget https://downloads.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz
# 解压
tar -xzvf hadoop-3.2.4.tar.gz
# /usr/local通常被用作本地软件安装目录。
mv hadoop-3.2.4 /usr/local/hadoop
```

3. 配置环境变量。
```shell
# ~/.bashrc是一个 Bash shell（Bourne - Again Shell）的配置文件。 
# 当你安装了一个新的软件，并且希望在命令行中能够方便地访问这个软件的可执行命令，就可以在~/.bashrc文件中设置PATH环境变量。
vi ~/.bashrc

# 在文件末尾添加以下内容：
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

# 重新刷新命令行环境变量。
source ~/.bashrc
```

